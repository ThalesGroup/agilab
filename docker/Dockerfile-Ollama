FROM ubuntu:24.04
LABEL authors="Julien BESTARD"
LABEL description="AGILAB Offline LLM container with Ollama"

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    ca-certificates \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Create necessary directories
RUN mkdir -p /root/.ollama \
    && mkdir -p /root/log

# Expose Ollama API port
EXPOSE 11434

# Create startup script that starts Ollama and pulls models
RUN echo '#!/bin/bash\n\
set -e\n\
echo "Starting Ollama service..."\n\
ollama serve > /root/log/ollama_serve.log 2>&1 &\n\
OLLAMA_PID=$!\n\
echo "Ollama PID: $OLLAMA_PID"\n\
\n\
# Wait for Ollama to be ready\n\
echo "Waiting for Ollama to be ready..."\n\
for i in {1..30}; do\n\
    if curl -sf http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then\n\
        echo "Ollama is ready!"\n\
        break\n\
    fi\n\
    echo "Waiting... ($i/30)"\n\
    sleep 2\n\
done\n\
\n\
# Pull models if they do not exist\n\
echo "Checking for mistral:instruct model..."\n\
if ! ollama list | grep -q "mistral:instruct"; then\n\
    echo "Pulling mistral:instruct model (this may take a while)..."\n\
    ollama pull mistral:instruct > /root/log/ollama_pull_mistral.log 2>&1\n\
    echo "Mistral model pulled successfully"\n\
else\n\
    echo "Mistral model already exists"\n\
fi\n\
\n\
echo "Checking for gpt-oss:20b model..."\n\
if ! ollama list | grep -q "gpt-oss:20b"; then\n\
    echo "Pulling gpt-oss:20b model (this may take a while)..."\n\
    ollama pull gpt-oss:20b > /root/log/ollama_pull_gpt-oss.log 2>&1\n\
    echo "GPT-OSS model pulled successfully"\n\
else\n\
    echo "GPT-OSS model already exists"\n\
fi\n\
\n\
echo "All models ready. Ollama is now serving..."\n\
# Keep the container running\n\
wait $OLLAMA_PID\n\
' > /usr/local/bin/start-ollama.sh && chmod +x /usr/local/bin/start-ollama.sh

# Start Ollama service
CMD ["/usr/local/bin/start-ollama.sh"]
